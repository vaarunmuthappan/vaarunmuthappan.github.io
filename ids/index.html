<!-- index.html -->
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to DS</title>
    <link rel="stylesheet" href="../styles.css">
</head>

<body>
    <header>
        <nav class="header-nav">
            <a href="../index.html" class="logo">Notes</a>
            <div class="nav-right">
                <a href="../courses.html">Topics</a>
                <a href="../tags.html">Tags</a>
            </div>
        </nav>
    </header>

    <div class="page-layout">
        <aside class="sidebar">
            <div class="toc">
                <h2>TABLE OF CONTENTS</h2>
                <ul>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#probability">Basic Probability</a></li>
                    <li><a href="#hypothesis-testing">Hypothesis Testing</a></li>
                    <li><a href="#significance_test">Statistical Tests</a></li>
                    <li><a href="#conclusion">Conclusion</a></li>
                </ul>
            </div>
        </aside>

        <main class="content notes">
            <div class="course-header">
                <h1>Introduction to DS</h1>
                <div class="course-meta">
                    <span>December 2024</span> ·
                    <a href="../courses.html" class="course-portal">Topic portal</a>
                </div>
            </div>

            <section id="introduction">
                <h2>Introduction</h2>
                <p>Notes covering basic data science topics.</p>
            </section>
            <section id="probability">
                The <strong>sample space</strong> (Omega) is the set of all possible outcomes from one random
                <strong>experiment</strong>, which is something repeatable. The <strong>event space</strong> is the set
                of
                all possible events, where an <strong>event</strong> is a set of outcomes. A <strong>probability
                    measure</strong> maps the event space to a number from 0 to 1. The <strong>probability
                    space</strong>
                consists of the sample space, event space, and probability measure.

                A <strong>random variable</strong> is a function that maps the sample space to real numbers.
                <strong>Probability distributions</strong> describe the likelihood of different outcomes.
                <strong>Axioms</strong> are things we assume to be true. The three axioms of probability are:

                1. Probabilities are positive real numbers (≥ 0)
                2. The probability of an outcome in the sample space (Omega) is 1
                3. For disjoint subsets, the probability of their union is the sum of their individual probabilities

                The <strong>addition rule</strong> states that the probability of a union b is p(a) + p(b) - p(a and b).
                The
                <strong>multiplication rule</strong> for independent events is p(a) * p(b), and for dependent events is
                p(a)
                * p(b|a). <strong>Conditional probability</strong> p(b|a) is the probability of b given a, calculated as
                p(b
                intersection a) / p(a).

                The <strong>normal distribution</strong> occurs when there are many independent events. It resembles the
                Pascal distribution in how different points are reached. <strong>Moments</strong> characterize the shape
                of
                a distribution, with the equation being the integration of x^k multiplied by the density(x). They come
                from
                moment generating functions that converge, though some do not.

                The first (raw) moment is E[x], the balancing point. The second (centralized) moment is variance. The
                third
                (centralized) moment, cubed deviation, indicates symmetry. The fourth (centralized) moment, kurtosis,
                describes the heaviness of tails compared to the normal distribution. There are more than five moments,
                with
                the Edgeworth series using hyper-skew moments (5th power) as an example.
                </p>
            </section>


            <section id="hypothesis-testing">
                <h2>Hypothesis Testing</h2>
                <p> The <strong>normal distribution</strong> is symmetric and only requires the first two moments to
                    describe it. It results from a combination of independent events. In contrast, the <strong>Cauchy
                        distribution</strong> does not converge, with undefined expected value and variance, as well as
                    undefined higher raw moments. It is characterized by fat tails.
                    text
                    <strong>Hypothesis testing</strong> relies on several important principles:

                    The <strong>Weak Law of Large Numbers</strong> states that for random sampling from independent and
                    identically distributed (IID) variables with finite mean and variance, the probability of the
                    difference between the sample mean deviation and population mean being larger than a small epsilon
                    approaches zero. This applies only to probability and allows for individual deviations.

                    The <strong>Central Limit Theorem (CLT)</strong>, attributed to Laplace, states that with repeated
                    resampling, the distribution of sample means becomes normal as sample size increases, regardless of
                    the underlying population distribution, provided it has finite mean and variance. This applies to
                    the distribution of sample means, not the sample itself, and begins to take effect as soon as the
                    sample size increases.

                    <strong>Random sampling</strong> ensures every member of the population has an equal chance of being
                    chosen, avoiding sampling bias. The <strong>standard error of the mean (SEM)</strong> is the
                    standard deviation of sample means divided by the square root of the sample size. To decrease the
                    SEM, the sample size must be increased by the square of the desired reduction factor.

                    It's important to note that the CLT may not apply if the variance is very large or if extreme values
                    are present, as sample means may not distribute normally in these cases. Despite typically having
                    only one sample in practice, the CLT is relevant because it ensures that deviations from the true
                    mean decrease rapidly, allowing us to be confident that a single sample mean is a close
                    approximation of the population mean.

                    A common misconception is that the CLT only converges at a sample size of 30. In reality,
                    convergence begins as soon as the sample size increases from 1 to 2. The sample size of 30 is not
                    universally sufficient, especially for unusual distributions. <strong>Cauchy distributions</strong>
                    become a concern when "black swan" events occur.

                    Finally, it's worth noting that the mean minimizes the squared L2 distance between points, while the
                    median minimizes the L1 distance (absolute distance).
                </p>
            </section>

            <section id="significance_tests">
                <h2>Significance Tests</h2>
                <p>This course covers fundamental concepts in Data Science...</p>
            </section>

            <section id="Power">
                <h2>SECTION 3</h2>
                <p>This course covers fundamental concepts in Data Science...</p>
            </section>


            <!-- Add other sections as needed -->
        </main>
    </div>
</body>

</html>