<!-- index.html -->
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to DS</title>
    <link rel="stylesheet" href="../styles.css">
</head>

<body>
    <header>
        <nav class="header-nav">
            <a href="../index.html" class="logo">Notes</a>
            <div class="nav-right">
                <a href="../courses.html">Topics</a>
                <a href="../tags.html">Tags</a>
            </div>
        </nav>
    </header>

    <div class="page-layout">
        <aside class="sidebar">
            <div class="toc">
                <h2>TABLE OF CONTENTS</h2>
                <ul>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#probability">Basic Probability</a></li>
                    <li><a href="#hypothesis_testing">Hypothesis Testing</a></li>
                    <li><a href="#significance-testing">Statistical Tests</a></li>
                    <li><a href="#conclusion">Conclusion</a></li>
                </ul>
            </div>
        </aside>

        <main class="content notes">
            <div class="course-header">
                <h1>Introduction to DS</h1>
                <div class="course-meta">
                    <span>December 2024</span> Â·
                    <a href="../courses.html" class="course-portal">Topic portal</a>
                </div>
            </div>

            <section id="introduction">
                <h2>Introduction</h2>
                <p>Notes covering basic data science topics.</p>
            </section>
            <div class="collapsible-section" id="probability">
                <h2 class="section-header">Significance Testing</h2>
                <div class="section-content">
                    <p><strong>Inference</strong> means making conclusions from data. We need probability because formal
                        logic has limitations in describing real-life situations where everything is uncertain. Natural
                        language is too subjective, so we use math to describe uncertainty through probability.</p>

                    <p><strong>Sample Space</strong> (Omega) is the set of all possible outcomes from one random
                        experiment.
                        An <strong>experiment</strong> is something repeatable. The <strong>event space</strong> is the
                        set
                        of all events possible, while an <strong>event</strong> is a set of outcomes. A
                        <strong>probability
                            measure</strong> maps the event space to a number from 0 to 1. The <strong>probability
                            space</strong> consists of the sample space, event space, and probability measure.
                    </p>

                    <p>A <strong>random variable</strong> is a function that maps the sample space to real numbers.
                        <strong>Axioms</strong> are things we assume to be true.
                    </p>

                    <p><strong>Three Axioms of Probability:</strong></p>
                    <p><strong>Axiom 1:</strong> Probabilities are positive real numbers, greater than or equal to 0.
                    </p>
                    <p><strong>Axiom 2:</strong> The probability that an outcome is in the sample space (Omega) equals
                        1.
                    </p>
                    <p><strong>Axiom 3:</strong> For disjoint subsets, the probability of their union is the sum of
                        their
                        individual probabilities.</p>

                    <p><strong>Addition rule:</strong> The probability of a union b is p(a) + p(b) - p(a and b).</p>

                    <p><strong>Multiplication rule:</strong> If events are independent, it's p(a) * p(b). If not
                        independent, it's p(a) * p(b|a).</p>

                    <p><strong>Conditional probability:</strong> p(b | a) is the probability of b given a, calculated as
                        p(b
                        intersection a) / p(a). The sample space shrinks to a, so of all probability of a, we need the
                        part
                        where b also happens.</p>

                    <p><strong>Normal distribution</strong> occurs if there are lots of events and each are independent
                        events. The way to get to different points in a Pascal distribution is like a normal
                        distribution.
                    </p>

                    <p><strong>Moments</strong> characterize the shape of a distribution. The equation is the
                        integration of
                        x^k * density(x). Moments come from moment generating functions that converge, though some do
                        not.
                    </p>

                    <p><strong>First moment (raw):</strong> E[x] is the balancing point. These are raw because they're
                        centered on the origin.</p>

                    <p><strong>Second moment (centralized):</strong> Variance, calculated by subtracting the mean.</p>

                    <p><strong>Third moment (centralized):</strong> Cubed deviation. If equal, it's symmetric;
                        otherwise,
                        it's asymmetric.</p>

                    <p><strong>Fourth moment (centralized):</strong> Kurtosis, which indicates heavy tails compared to a
                        normal distribution. In a normal distribution, it drops off extremely and exponentially with
                        increasing n.</p>

                    <p>There are 5+ moments, but these are the most common. For example, the Edgeworth series has hyper
                        skew
                        moments to the 5th power.</p>
                </div>
            </div>


            <section id="hypothesis_testing">
                <h2>Hypothesis Testing</h2>

                <p>The <strong>normal distribution</strong> is symmetric and only requires the first two moments. It
                    results from a combination of independent events. In contrast, the <strong>Cauchy
                        distribution</strong> does not converge, with undefined expected value and variance. It has fat
                    tails, and higher raw moments are also not defined.</p>

                <p><strong>Weak law of large numbers:</strong> For random sampling from independent and identically
                    distributed (IID) variables with finite mean and variance, the probability of the difference between
                    the sample mean deviation and population mean being larger than a small epsilon approaches zero.
                    This applies only to probability and allows for individual deviations.</p>

                <p><strong>Strong law of large numbers:</strong> This concept originates from Bernoulli's work.</p>

                <p><strong>Central Limit Theorem (CLT):</strong> Developed by Laplace, it states that if resampling is
                    continually performed, the sample means distribute normally as sample size increases, regardless of
                    the underlying population, provided there is a finite mean and variance. This applies to the
                    distribution of sample means, not the sample itself, and begins as soon as the sample size
                    increases.</p>

                <p><strong>Sampling randomly</strong> means every member of the population has an equal chance of being
                    chosen. The opposite is sampling bias. The value in increasing sample size above 30 is that the
                    standard error of the mean decreases.</p>

                <p><strong>Standard error of the mean</strong> is the standard deviation of sample means with n
                    different samples divided by the square root of n. To decrease the standard error of the mean, the
                    sample size must be increased by the square of the desired reduction factor.</p>

                <p>If variance is not infinite but very large, such as with extreme values, the sample means may not
                    distribute normally. In practice, we typically have only one sample, not multiple samples of size n.
                    The CLT is relevant because it ensures that deviations from the true mean decrease rapidly, allowing
                    us to be confident that one sample is as close as we can get to the population mean.</p>

                <p><strong>Misconception:</strong> The CLT doesn't converge at 30; it starts converging as soon as we
                    increase the sample size, even from 1 to 2. The notion that 30 is sufficient is not true for some
                    unusual distributions.</p>

                <p>Cauchy distributions become a concern when black swan events occur. It's worth noting that the mean
                    minimizes the squared L2 distance between points, while the median minimizes the L1 distance
                    (absolute).</p>
            </section>

            <section id="significance-testing">
                <h2>Significance Testing</h2>

                <p>In real-life scenarios, we implement experiments through A/B tests, as it's often impractical to
                    conduct traditional experiments on humans. A/B testing applies experimental procedures to real-life
                    situations, allowing us to draw causal conclusions.</p>

                <p><strong>Real experiments</strong> involve creating conditions, randomly assigning users to different
                    treatments (independent variables), and measuring outcomes. Given a large number of users,
                    randomization can control for pre-existing relationships. The underlying theory assumes an equal
                    proportion of confounds in each treatment, which is more likely with a large sample size.</p>

                <p><strong>Statistical significance</strong> indicates that, given the null hypothesis is true, the
                    observation is unlikely due to chance alone[1]. It's important to note that this is a statement
                    about the data, not a conclusion. Statistical significance occurs when the probability of the data
                    is below a chosen level, assuming the null hypothesis is true.</p>

                <p>Invented by Ronald Fisher, statistical significance is expressed as p(data | null hypothesis), not
                    p(hypothesis | data). However, it can be inverted using Bayes' theorem and prior probabilities[8].
                </p>

                <h3>Null Hypothesis Significance Testing (NHST)</h3>

                <p>NHST is based on falsification. The process involves:</p>
                <ol>
                    <li>Assuming the opposite of your hypothesis is true (null hypothesis)</li>
                    <li>Collecting data</li>
                    <li>Proving that the data is so unlikely under the null hypothesis that it must be rejected,
                        supporting your alternative hypothesis</li>
                </ol>

                <p>It's crucial to remember that there's always a chance that sampling error could produce unlikely
                    data[2].</p>

                <h3>Binomial Example</h3>

                <p>In a binomial test, we stack all results and calculate the probability using the combination formula:
                    nCk for all k >= min[10]. This approach is particularly useful when dealing with two possible
                    outcomes (success or failure) and you have an idea about the probability of success[7].</p>

                <div class="fact-card">
                    <h3>Interesting Fact</h3>
                    <p>The commonly used significance level of 0.05 (1 in 20 chance) originated from Fisher, who found
                        it convenient as calculators were not available at the time. This highlights that the choice of
                        alpha (significance level) is somewhat arbitrary[6].</p>
                </div>
            </section>



            <!-- Add other sections as needed -->
        </main>
    </div>
    <script src="../script.js"></script>
</body>

</html>