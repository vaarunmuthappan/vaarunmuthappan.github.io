<!-- index.html -->
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to DS</title>
    <link rel="stylesheet" href="../styles.css">
</head>

<body>
    <header>
        <nav class="header-nav">
            <a href="../index.html" class="logo">Notes</a>
            <div class="nav-right">
                <a href="../courses.html">Topics</a>
                <a href="../tags.html">Tags</a>
            </div>
        </nav>
    </header>

    <div class="page-layout">
        <aside class="sidebar">
            <div class="toc">
                <h2>TABLE OF CONTENTS</h2>
                <ul>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#significance-testing">Significance Testing</a></li>
                    <li><a href="#significance-problems">Problems with p-values</a></li>
                    <li><a href="#power-and-confidence">Power and Confidence</a></li>
                    <li><a href="#bayesian-inference">Bayesian Inference</a></li>
                    <li><a href="#machine-learning-regression">Machine Learning - Regression</a></li>
                    <li><a href="#ml-regularization">Machine Learning - Regularization</a></li>
                    <li><a href="#ml-classification">Machine Learning - Classification</a></li>
                    <li><a href="#ml-unsupervised-learning">Machine Learning - Unsupervised Learning</a></li>
                </ul>
            </div>
        </aside>

        <main class="content notes">
            <div class="course-header">
                <h1>Introduction to DS</h1>
                <div class="course-meta">
                    <span>December 2024</span> Â·
                    <a href="../courses.html" class="course-portal">Topic portal</a>
                </div>
            </div>

            <section id="introduction">
                <h2>Introduction</h2>
                <p>Notes covering basic data science topics.</p>
            </section>
            <div class="collapsible-section" id="probability">
                <h2 class="section-header">Probability </h2>
                <div class="section-content">
                    <p><strong>Inference</strong> means making conclusions from data. We need probability because formal
                        logic has limitations in describing real-life situations where everything is uncertain. Natural
                        language is too subjective, so we use math to describe uncertainty through probability.</p>

                    <p><strong>Sample Space</strong> (Omega) is the set of all possible outcomes from one random
                        experiment.
                        An <strong>experiment</strong> is something repeatable. The <strong>event space</strong> is the
                        set
                        of all events possible, while an <strong>event</strong> is a set of outcomes. A
                        <strong>probability
                            measure</strong> maps the event space to a number from 0 to 1. The <strong>probability
                            space</strong> consists of the sample space, event space, and probability measure.
                    </p>

                    <p>A <strong>random variable</strong> is a function that maps the sample space to real numbers.
                        <strong>Axioms</strong> are things we assume to be true.
                    </p>

                    <p><strong>Three Axioms of Probability:</strong></p>
                    <p><strong>Axiom 1:</strong> Probabilities are positive real numbers, greater than or equal to 0.
                    </p>
                    <p><strong>Axiom 2:</strong> The probability that an outcome is in the sample space (Omega) equals
                        1.
                    </p>
                    <p><strong>Axiom 3:</strong> For disjoint subsets, the probability of their union is the sum of
                        their
                        individual probabilities.</p>

                    <p><strong>Addition rule:</strong> The probability of a union b is p(a) + p(b) - p(a and b).</p>

                    <p><strong>Multiplication rule:</strong> If events are independent, it's p(a) * p(b). If not
                        independent, it's p(a) * p(b|a).</p>

                    <p><strong>Conditional probability:</strong> p(b | a) is the probability of b given a, calculated as
                        p(b
                        intersection a) / p(a). The sample space shrinks to a, so of all probability of a, we need the
                        part
                        where b also happens.</p>

                    <p><strong>Normal distribution</strong> occurs if there are lots of events and each are independent
                        events. The way to get to different points in a Pascal distribution is like a normal
                        distribution.
                    </p>

                    <p><strong>Moments</strong> characterize the shape of a distribution. The equation is the
                        integration of
                        x^k * density(x). Moments come from moment generating functions that converge, though some do
                        not.
                    </p>

                    <p><strong>First moment (raw):</strong> E[x] is the balancing point. These are raw because they're
                        centered on the origin.</p>

                    <p><strong>Second moment (centralized):</strong> Variance, calculated by subtracting the mean.</p>

                    <p><strong>Third moment (centralized):</strong> Cubed deviation. If equal, it's symmetric;
                        otherwise,
                        it's asymmetric.</p>

                    <p><strong>Fourth moment (centralized):</strong> Kurtosis, which indicates heavy tails compared to a
                        normal distribution. In a normal distribution, it drops off extremely and exponentially with
                        increasing n.</p>

                    <p>There are 5+ moments, but these are the most common. For example, the Edgeworth series has hyper
                        skew
                        moments to the 5th power.</p>
                </div>
            </div>

            <div class="collapsible-section" id="hypothesis-testing">
                <h2 class="section-header">Hypothesis Testing</h2>
                <div class="section-content">
                    <p><strong>Normal distribution:</strong> Symmetric, only requiring the first two moments. It results
                        from the combination of independent events.</p>
                    <p><strong>Distribution that does not converge:</strong> The Cauchy distribution. It has undefined
                        expected value and variance, with higher raw moments also undefined. It is characterized by fat
                        tails.</p>

                    <h3>Hypothesis Testing:</h3>

                    <p><strong>Weak Law of Large Numbers:</strong> For random sampling from independent and identically
                        distributed (IID) variables with finite mean and variance (excluding black swan events like
                        Cauchy distribution), the probability of the difference between the sample mean deviation and
                        population mean being larger than a small epsilon approaches zero. This applies only to
                        probability and individual deviations can still occur.</p>

                    <p><strong>Central Limit Theorem (CLT):</strong> With repeated resampling, the distribution of
                        sample means becomes normal as sample size increases, regardless of the underlying population,
                        provided it has finite mean and variance. This phenomenon occurs due to the cancellation of
                        deviations.</p>

                    <ul>
                        <li>The CLT applies to the distribution of sample means, not the sample itself.</li>
                        <li>It begins to take effect as soon as the sample size increases.</li>
                        <li>Increasing sample size beyond 30 decreases the standard error of the mean.</li>
                    </ul>

                    <p><strong>Random Sampling:</strong> Every member of the population has an equal chance of being
                        chosen. The opposite is sampling bias.</p>

                    <p><strong>Standard Error of the Mean (SEM):</strong> Calculated as the standard deviation of sample
                        means divided by the square root of the sample size. To decrease SEM, the sample size must be
                        increased by the square of the desired reduction factor.</p>

                    <p><strong>Practical Implications:</strong> In real-life scenarios, we typically have only one
                        sample, not multiple samples of size n. The CLT is relevant because it ensures that deviations
                        from the true mean decrease with the square root of the exponential, allowing us to be confident
                        that a single sample is as close as possible to the population mean.</p>

                    <p><strong>Common Misconception:</strong> The CLT doesn't suddenly converge at a sample size of 30.
                        It begins converging as soon as the sample size increases, even from 1 to 2. The notion that 30
                        is sufficient isn't universally true, especially for certain unusual distributions.</p>

                    <p><strong>Cauchy Distribution Concerns:</strong> This becomes relevant when dealing with black swan
                        events.</p>

                    <p><strong>Mean vs. Median:</strong> The mean minimizes the squared L2 distance between points,
                        while the median minimizes the L1 distance (absolute).</p>
                </div>
            </div>

            <div class="collapsible-section" id="significance-testing">
                <h2 class="section-header">Significance Testing</h2>
                <div class="section-content">
                    <p><strong>A/B Testing:</strong> We implement experiments in real life using A/B tests, as it's
                        often impractical to conduct experiments directly on humans.</p>
                    text
                    <h3>Real Experiments:</h3>
                    <ul>
                        <li>Create conditions</li>
                        <li>Randomly assign users to different treatments (Treatments = Independent variable)</li>
                        <li>Measure outcomes</li>
                    </ul>

                    <p><strong>Randomization:</strong> Given a large number of users, randomization can control for
                        pre-existing relationships. The underlying theory assumes an equal proportion of confounds in
                        each treatment, which is more likely with a large sample size.</p>

                    <p><strong>Causality:</strong> Real experiments allow for conclusions about causality. However, in
                        real life, randomly assigning people is often challenging due to logistics, ethics, or other
                        constraints.</p>

                    <p><strong>A/B Tests:</strong> These apply experimental procedures to real-life situations, allowing
                        for practical implementation of controlled experiments.</p>

                    <h3>Statistical Significance:</h3>

                    <p><strong>Definition:</strong> Given that the null hypothesis is true, the observation is unlikely
                        due to chance alone. This is a statement about data, not a conclusion. It means the probability
                        of the data is below a chosen level, assuming the null hypothesis is true.</p>

                    <p><strong>Origin:</strong> Invented by Fisher.</p>

                    <p><strong>Interpretation:</strong> It represents p(data | null hypothesis), not p(hypothesis |
                        data). However, it can be inverted using Bayes' theorem and prior probabilities.</p>

                    <p><strong>Caution:</strong> There's always a chance that sampling error alone could produce
                        unlikely data.</p>

                    <h3>Null Hypothesis Significance Testing:</h3>

                    <p>Based on falsification, this method assumes the opposite is true, collects data, and then proves
                        that the result is so unlikely that the null hypothesis must be false, supporting the
                        alternative hypothesis.</p>

                    <p><strong>Binomial Example:</strong> Stack all results (details to be added).</p>

                    <p><strong>Interesting Fact:</strong> The commonly used significance level of 0.05 (1 in 20 chance)
                        originated from Fisher, who found it convenient in an era without calculators. Thus, the choice
                        of alpha (significance level) is somewhat arbitrary.</p>
                </div>
            </div>

            <div class="collapsible-section" id="parametric-tests">
                <h2 class="section-header">Parametric Significance Tests</h2>
                <div class="section-content">
                    <p>In parametric significance tests, you reject the null hypothesis if the difference is too large
                        to be plausibly consistent with chance. These tests reduce to a distribution with a parameter,
                        usually the mean.</p>
                    text
                    <h3>Methodology:</h3>
                    <ul>
                        <li>Establish null hypothesis</li>
                        <li>Calculate test statistic</li>
                        <li>Determine likelihood of statistic under null hypothesis (p-value)</li>
                    </ul>

                    <p>The <strong>p-value</strong> is the probability of observing the data assuming the null
                        hypothesis is true. For countable data, binomial distribution or combinatorics can be used to
                        find this probability.</p>

                    <p>All tests measure how far the data is from the population mean in units of Standard Error of the
                        Mean (SEM).</p>

                    <p><strong>Misconception:</strong> Normalization doesn't necessarily mean the data becomes normally
                        distributed. It's primarily used to compare different scores.</p>

                    <h3>Types of Errors:</h3>
                    <p><strong>Type 1 Error (False Positive):</strong> Concluding significance when there isn't any in
                        reality. This could be due to sampling error.</p>

                    <p><strong>Type 2 Error (False Negative):</strong> Failing to detect significance when it exists.
                        This can occur if the sampling error is too large, and can be mitigated by increasing sample
                        size.</p>

                    <h3>Z-test:</h3>
                    <p>Z = (sample mean - population mean) / SEM</p>
                    <p>SEM = population sd / ân</p>

                    <p>The Z-score is converted to a probability using the standard normal distribution (mean = 0, sd =
                        1). However, this test has limitations: it usually requires knowledge of population mean and SD,
                        and relies on the Central Limit Theorem.</p>

                    <h3>Degrees of Freedom (DOF):</h3>
                    <p><strong>Misconception:</strong> DOF is not always n-1.</p>
                    <p>DOF represents the number of independent pieces of information in the dataset. It's calculated as
                        n - k, where k is the number of calculations performed.</p>

                    <h3>T-test:</h3>
                    <p>Used for small sample sizes and unknown population parameters. As DOF increases, the
                        t-distribution approaches the z-distribution.</p>

                    <p>T statistic = (sample mean 1 - sample mean 2) / SEMpooled</p>

                    <p>Assumptions: mean is meaningful, data is normally distributed, homogeneity of variance.</p>

                    <p>Two versions:</p>
                    <ul>
                        <li>T-test for 2 independent groups (DOF = n1 + n2 - 2)</li>
                        <li>T-test for 1 correlated group (DOF = N - 1)</li>
                    </ul>

                    <h3>Welch's t-test:</h3>
                    <p>Preferred when homogeneity of variance can't be assumed. Unlike Student's t-test, it doesn't pool
                        variance, leading to fractional DOF.</p>

                    <h3>ANOVA:</h3>
                    <p>Extends t-test logic to more groups. Equivalent to multiple regression.</p>
                </div>
            </div>

            <div class="collapsible-section" id="non-parametric-tests">
                <h2 class="section-header">Non-parametric Significance Tests</h2>
                <div class="section-content">
                    <p>Non-parametric tests are needed when data doesn't lend itself to sample means, such as with
                        categorical data or ratings.</p>
                    <h3>Types of Data:</h3>
                    <ul>
                        <li><strong>Categorical:</strong> Sets with common attributes, only having nominality.</li>
                        <li><strong>Ratings:</strong> Discrete but not categorical. They have nominality and ordinality,
                            but not cardinality.</li>
                    </ul>

                    <p><strong>Properties of Numbers:</strong></p>
                    <ul>
                        <li>Nominality</li>
                        <li>Cardinality (equal distance?)</li>
                        <li>Ordinality</li>
                    </ul>

                    <p>Non-parametric tests typically require higher sample sizes but have fewer assumptions.</p>

                    <h3>Chi-squared Test:</h3>
                    <p>Used for categorical counts or frequencies. It compares observed data patterns with expected
                        frequency counts under the null hypothesis.</p>
                    <p>Test statistic: sum of ((differences squared) / expected Count)</p>
                    <p>Degrees of freedom (DOF) = number of categories - 1</p>

                    <h3>Mann-Whitney U Test:</h3>
                    <p>Also known as Mann-Whitney Wilcoxon test or ranksum test. It tests whether two samples come from
                        populations with the same median.</p>
                    <p>Procedure: Rank all data from smallest to largest; sum of ranks for both datasets should be
                        similar.</p>

                    <h3>Kolmogorov-Smirnov (KS) Test:</h3>
                    <p>Compares underlying distributions via the empirical Cumulative Distribution Function (CDF).</p>
                    <p>Procedure: Finds maximum distance between two distributions and compares to the null hypothesis
                        that distributions are the same.</p>

                    <h3>Kruskal-Wallis Test:</h3>
                    <p>Non-parametric version of ANOVA for 3+ groups.</p>

                    <h3>Permutation Test Approach:</h3>
                    <p>For unusual cases, you can create your own test using this approach:</p>
                    <ol>
                        <li>Develop a statistic based on domain knowledge</li>
                        <li>Sample without replacement</li>
                        <li>Calculate the statistic for all groups</li>
                        <li>Determine the p-value</li>
                    </ol>
                    <p>Results should be robust to details of the test statistic.</p>
                </div>

            </div>
            <div class="collapsible-section" id="p-value-problems">
                <h2 class="section-header">Problems with p-values</h2>
                <div class="section-content">
                    <p>Decisions based solely on statistical significance can be misleading. P-values merely indicate
                        the unlikelihood of data assuming the null hypothesis is true.</p>

                    <h3>Contextualizing p-values:</h3>
                    <p>P-values need to be considered alongside:</p>
                    <ul>
                        <li>Effect Size</li>
                        <li>Statistical Power</li>
                        <li>Confidence</li>
                    </ul>

                    <p><strong>Replicability Crisis:</strong> Many results fail to replicate, primarily due to lack of
                        statistical power. Root causes include small sample sizes, small effects, negative results, and
                        incentives to publish positive results.</p>

                    <h3>P-hacking:</h3>
                    <ul>
                        <li><strong>Flexible stopping:</strong> Adding observations until reaching the significance
                            threshold, leading to alpha inflation.</li>
                        <li><strong>HARKing (Hypothesis After Results are Known):</strong> Developing hypotheses based
                            on which variables show significance.</li>
                        <li><strong>Removal of outliers:</strong> Should only be done if the outlier is known to be
                            erroneous.</li>
                    </ul>

                    <h3>Effect Size:</h3>
                    <p>Measures the practical significance of results. For example, Cohen's d = (pop 1 mean - pop mean
                        2) / sd of population.</p>
                    <p>Typical effect sizes in psychology are around 0.2 to 0.25.</p>

                    <p><strong>Effect:</strong> The existence of a difference between treatment and control.</p>
                    <p><strong>Effect size:</strong> The magnitude of the real difference in populations.</p>

                    <p>Effect size is independent of sample size, but the ability to detect it relates to sample size
                        through statistical power.</p>
                </div>
            </div>

            <div class="collapsible-section" id="power-and-confidence">
                <h2 class="section-header">Power and Confidence</h2>
                <div class="section-content">
                    <h3>Statistical Power</h3>
                    <p><strong>Definition:</strong> The probability of detecting an effect of a given size if the effect
                        exists. Mathematically, it's expressed as 1 - Î², where Î² is the probability of making a Type II
                        error[1][5].</p>
                    <p>Typically, a power of at least 0.8 (Î² = 0.2) is desired. Power is implicitly set by sample size
                        and effect size[1].</p>

                    <p><strong>Importance of Power:</strong></p>
                    <ul>
                        <li>Determines adequate sample size</li>
                        <li>Allows absence of evidence to become evidence of absence</li>
                        <li>Helps prevent real effects from being overwhelmed by sampling error</li>
                    </ul>

                    <h3>Factors Affecting Power</h3>
                    <ul>
                        <li><strong>Alpha level:</strong> Higher alpha increases power by accepting more H1</li>
                        <li><strong>Sample size:</strong> Larger samples increase power non-linearly (ân relationship)
                        </li>
                        <li><strong>Effect size:</strong> Larger effects increase power due to greater distance between
                            means</li>
                        <li><strong>Type of test:</strong> Pairwise t-test > independent t-test > parametric >
                            non-parametric</li>
                    </ul>

                    <p><strong>Power Curve:</strong> In psychology, real effects are typically around 0.2. The rate of
                        significant findings should be 10%-30%, but is often reported as 95% in practice[7].</p>

                    <p><strong>Legitimate Power Increase:</strong> Using a one-tailed test can increase power by
                        adjusting alpha.</p>

                    <p><strong>A Priori vs. Post Hoc Power:</strong> Only a priori power analysis is considered valid.
                        Post-hoc (observed) power can be misleading due to potential discrepancies between true and
                        sample effect sizes[3].</p>

                    <h3>Confidence Intervals</h3>
                    <p><strong>Definition:</strong> A range that likely contains the true population parameter[8].</p>

                    <p>Confidence intervals visualize sampling error. A 95% CI means that if the study were repeated,
                        95% of the time the parameter would fall within the interval, assuming only sampling error[8].
                    </p>

                    <p><strong>Calculation:</strong> CI can be calculated using z*SEM if the Central Limit Theorem (CLT)
                        holds. If CLT cannot be proven, bootstrap methods are used[6].</p>

                    <h3>Bootstrap</h3>
                    <p>A resampling technique used to estimate confidence intervals when normal distribution assumptions
                        don't hold[6].</p>

                    <p><strong>Caution:</strong> Bootstrap can be dangerous with small samples as it may overrepresent
                        parameters. It's only effective if the entire sample is representative of the population[6].</p>

                    <p><strong>Limitation:</strong> Bootstrapping provides more numbers but not more data, and does not
                        increase degrees of freedom[6].</p>
                </div>
            </div>

            <div class="collapsible-section" id="bayesian-inference">
                <h2 class="section-header">Bayesian Inference</h2>
                <div class="section-content">
                    <p>Bayesian inference is a statistical approach that updates beliefs based on new evidence using
                        Bayes' theorem:</p>
                    <p>$$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} $$</p>

                    <p>Where:</p>
                    <ul>
                        <li><strong>Likelihood:</strong> $$P(B|A)$$ - probability of data given the hypothesis</li>
                        <li><strong>Prior:</strong> $$P(A)$$ - initial belief about the hypothesis</li>
                        <li><strong>Posterior:</strong> $$P(A|B)$$ - updated belief after observing the data</li>
                    </ul>

                    <h3>Bayesian vs. Frequentist Approach</h3>
                    <p><strong>Frequentist probability</strong> is based on the long-run frequency of outcomes, while
                        <strong>Bayesian probability</strong> represents a degree of belief. Bayesian inference allows
                        for updating prior beliefs with new information, which can be advantageous when prior knowledge
                        is well-founded[1].
                    </p>

                    <h3>Bayesian Hypothesis Testing</h3>
                    <p>Bayesian hypothesis testing allows for direct comparison of specific hypotheses, unlike the
                        traditional null hypothesis approach. The <strong>Bayes factor</strong> quantifies the relative
                        evidence for one hypothesis over another, calculated as the ratio of likelihoods[1].</p>

                    <h3>Bayesian Power Analysis</h3>
                    <p>The <strong>Positive Predictive Value (PPV)</strong> represents the post-study probability that a
                        significant result is true:</p>

                    <p>$$ PPV = \frac{P(True) \cdot Power}{P(True) \cdot Power + \alpha \cdot (1 - P(True))} $$</p>

                    <p>This formula highlights that the false positive rate depends on power, prior probability of a
                        true effect, and the chosen alpha level[1].</p>

                    <h3>Bayesian Credible Intervals</h3>
                    <p>In Bayesian statistics, parameters are treated as random variables with distributions. As sample
                        size increases, the likelihood dominates the prior, converging to frequentist results with
                        infinite data[1].</p>
                </div>
            </div>
            <div class="collapsible-section" id="machine-learning-regression">
                <h2 class="section-header">Machine Learning - Regression</h2>
                <div class="section-content">
                    <h3>Types of Machine Learning</h3>
                    <ul>
                        <li><strong>Supervised Learning:</strong> Regression and Classification</li>
                        <li><strong>Unsupervised Learning:</strong> Dimension Reduction and Clustering</li>
                        <li><strong>Reinforcement Learning</strong></li>
                    </ul>
                    <h3>Linear Regression</h3>
                    <p><strong>Regression</strong> models correlations, not causation. It minimizes the summed squared
                        distance (L2 norm) between predictions and actual values[8].</p>

                    <p><strong>Beta coefficient:</strong></p>
                    <ul>
                        <li>Scaling factor making independent variable closest to dependent variable</li>
                        <li>Interpreted as the change in standard deviations of DV for one standard deviation change in
                            IV</li>
                        <li>Calculated as $$ \beta = (X^TX)^{-1}X^Ty $$</li>
                    </ul>

                    <h3>Multiple Regression</h3>
                    <p>Allows comparison of multiple variables by accounting for confounds. It can help control for
                        variables when experiments are not possible[8].</p>

                    <h3>Model Evaluation</h3>
                    <p><strong>Coefficient of Determination (RÂ²):</strong> Proportion of variance explained by the model
                    </p>
                    <p>$$ R^2 = \frac{\text{Explained Variance}}{\text{Total Variance}} $$</p>

                    <p><strong>R:</strong> Correlation between predicted and actual values</p>

                    <p>Plotting predicted vs. actual values helps visualize model performance and RÂ²[8].</p>
                </div>
            </div>
            <div class="collapsible-section" id="ml-regularization">
                <h2 class="section-header">Machine Learning - Regularization</h2>
                <div class="section-content">
                    <p>Regularization aims to create models that not only account for maximum variance but are also as
                        simple as possible.</p>
                    <h3>Problems with Multiple Regression</h3>
                    <ul>
                        <li><strong>Multicollinearity:</strong> Predictors are correlated, reducing model reliability.
                            Solutions include using fewer predictors or regularization.</li>
                        <li><strong>Curse of dimensionality:</strong> Many predictors require lots of parameters,
                            leading to coverage issues. This problem diminishes with more data.</li>
                        <li><strong>Overfitting:</strong> Model fits noise in the data, reducing generalizability.
                            Cross-validation and checking RMSE on a test set can help detect this.</li>
                    </ul>

                    <h3>One-hot Encoding</h3>
                    <p>Converts categorical variables into separate boolean columns. To avoid the dummy variable trap,
                        one category should be removed to prevent collinearity.</p>

                    <h3>Bias-Variance Tradeoff</h3>
                    <p><strong>Bias:</strong> Model is too simple to capture the relationship (underfitting).</p>
                    <p><strong>Variance:</strong> Model doesn't generalize well (overfitting).</p>

                    <h3>Regularization Techniques</h3>
                    <p><strong>Ridge Regression:</strong> Adds Î» * Î²Â² to the minimization term, introducing some bias to
                        fit better to the population.</p>
                    <p><strong>Lasso Regression:</strong> Uses L1 norm penalty, which can reduce some coefficients to
                        zero.</p>
                </div>
            </div>
            <div class="collapsible-section" id="ml-classification">
                <h2 class="section-header">Machine Learning - Classification</h2>
                <div class="section-content">
                    <p>Classification aims to find boundaries that maximize distance between classes, unlike regression
                        which minimizes distance to points.</p>
                    <h3>Logistic Regression</h3>
                    <p>Maps continuous inputs to binary outcomes using a nonlinear function (sigmoid). It computes the
                        odds of two classes and predicts the inverse of the logit.</p>

                    <h3>ROC Curve</h3>
                    <p>A metric for classification performance that plots True Positive Rate against False Positive
                        Rate. The Area Under ROC (AUROC) quantifies overall performance, with 0.5 indicating random
                        guessing.</p>

                    <h3>Support Vector Machines (SVM)</h3>
                    <p>Finds the line/plane that maximizes the distance between groups. Soft margin SVM allows for some
                        misclassification with a penalty.</p>
                </div>

            </div>
    </div>
    <div class="collapsible-section" id="ml-unsupervised-learning">
        <h2 class="section-header">Machine Learning - Unsupervised Learning</h2>
        <div class="section-content">
            <h3>Dimensionality Reduction</h3>
            <p><strong>Principal Component Analysis (PCA):</strong> Finds orthonormal basis and projects
                data onto it, keeping only the most significant components. Variables should be standardized
                (z-score) before PCA.</p>
            <h3>Clustering</h3>
            <p><strong>K-means:</strong> Partitions data into K clusters. The optimal K can be chosen using
                the elbow method or silhouette method.</p>

            <p><strong>Silhouette Method:</strong> Calculates a score for each datapoint based on its
                distance to its own cluster versus other clusters. The K with the highest average silhouette
                score is chosen.</p>

            <p><strong>HDBSCAN:</strong> A density-based clustering method that identifies core, edge, and
                anomaly points.</p>
        </div>
    </div>

    </main>
    </div>
    <script src="../script.js"></script>
</body>

</html>