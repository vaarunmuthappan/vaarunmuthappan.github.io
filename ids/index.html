<!-- index.html -->
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to DS</title>
    <link rel="stylesheet" href="../styles.css">
</head>

<body>
    <header>
        <nav class="header-nav">
            <a href="../index.html" class="logo">Notes</a>
            <div class="nav-right">
                <a href="../courses.html">Topics</a>
                <a href="../tags.html">Tags</a>
            </div>
        </nav>
    </header>

    <div class="page-layout">
        <aside class="sidebar">
            <div class="toc">
                <h2>TABLE OF CONTENTS</h2>
                <ul>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#probability">Basic Probability</a></li>
                    <li><a href="#hypothesis-testing">Hypothesis Testing</a></li>
                    <li><a href="#significance_test">Statistical Tests</a></li>
                    <li><a href="#conclusion">Conclusion</a></li>
                </ul>
            </div>
        </aside>

        <main class="content notes">
            <div class="course-header">
                <h1>Introduction to DS</h1>
                <div class="course-meta">
                    <span>December 2024</span> ·
                    <a href="../courses.html" class="course-portal">Topic portal</a>
                </div>
            </div>

            <section id="introduction">
                <h2>Introduction</h2>
                <p>Notes covering basic data science topics.</p>
            </section>
            <section id="probability">
                <h2>Probability</h2>
                <p>
                    <strong>Inference</strong> means making conclusions from data. Probability is necessary because
                    formal logic has limitations, describing things as always or never true, while real life is
                    uncertain. Natural language is too subjective, so we need mathematics to describe uncertainty
                    through probability.
                </p>
                <p>
                    Key concepts in probability include:
                <ul>
                    <li><strong>Sample Space (Omega)</strong>: Set of all possible outcomes from one random experiment.
                    </li>
                    <li><strong>Experiment</strong>: Something repeatable.</li>
                    <li><strong>Event Space</strong>: Set of all possible events.</li>
                    <li><strong>Event</strong>: Set of outcomes.</li>
                    <li><strong>Probability Measure</strong>: Maps event space to a number from 0-1.</li>
                    <li><strong>Probability Space</strong>: Consists of sample space, event space, and probability
                        measure.</li>
                    <li><strong>Random Variable</strong>: Function that maps sample space to real numbers.</li>
                </ul>
                </p>
                <p>
                    <strong>Axioms</strong> are things we assume to be true. The three axioms of probability are:
                <ol>
                    <li>Probabilities are positive real numbers (≥ 0).</li>
                    <li>Probability of an outcome in the sample space (Omega) is 1.</li>
                    <li>For disjoint subsets, the probability of their union is the sum of their individual
                        probabilities.</li>
                </ol>
                </p>
                <p>
                    Important probability rules include:
                <ul>
                    <li><strong>Addition Rule</strong>: Probability of a union b is p(a) + p(b) - p(a and b).</li>
                    <li><strong>Multiplication Rule</strong>: For independent events, p(a) * p(b); for dependent events,
                        p(a) * p(b|a).</li>
                    <li><strong>Conditional Probability</strong>: p(b|a) = p(b given a) = p(b intersection a) / p(a).
                    </li>
                </ul>
                </p>
                <p>
                    The <strong>Normal Distribution</strong> occurs with many independent events. It resembles the
                    Pascal distribution in how different points are reached.
                </p>
                <p>
                    <strong>Moments</strong> characterize the shape of a distribution. The equation is the integration
                    of x^k * density(x). They come from moment generating functions that converge, though some do not.
                    Important moments include:
                <ul>
                    <li>1st (raw): E[x] = balancing point (centered on origin).</li>
                    <li>2nd (centralized): Variance (subtracting mean).</li>
                    <li>3rd (centralized): Cubed deviation (indicates symmetry).</li>
                    <li>4th (centralized): Kurtosis (describes tail heaviness compared to normal distribution).</li>
                </ul>
                There are more than five moments, with the Edgeworth series using hyper-skew moments (5th power) as an
                example.
                </p>
            </section>


            <section id="hypothesis-testing">
                <h2>Hypothesis Testing</h2>
                <p> The <strong>normal distribution</strong> is symmetric and only requires the first two moments to
                    describe it. It results from a combination of independent events. In contrast, the <strong>Cauchy
                        distribution</strong> does not converge, with undefined expected value and variance, as well as
                    undefined higher raw moments. It is characterized by fat tails.
                    text
                    <strong>Hypothesis testing</strong> relies on several important principles:

                    The <strong>Weak Law of Large Numbers</strong> states that for random sampling from independent and
                    identically distributed (IID) variables with finite mean and variance, the probability of the
                    difference between the sample mean deviation and population mean being larger than a small epsilon
                    approaches zero. This applies only to probability and allows for individual deviations.

                    The <strong>Central Limit Theorem (CLT)</strong>, attributed to Laplace, states that with repeated
                    resampling, the distribution of sample means becomes normal as sample size increases, regardless of
                    the underlying population distribution, provided it has finite mean and variance. This applies to
                    the distribution of sample means, not the sample itself, and begins to take effect as soon as the
                    sample size increases.

                    <strong>Random sampling</strong> ensures every member of the population has an equal chance of being
                    chosen, avoiding sampling bias. The <strong>standard error of the mean (SEM)</strong> is the
                    standard deviation of sample means divided by the square root of the sample size. To decrease the
                    SEM, the sample size must be increased by the square of the desired reduction factor.

                    It's important to note that the CLT may not apply if the variance is very large or if extreme values
                    are present, as sample means may not distribute normally in these cases. Despite typically having
                    only one sample in practice, the CLT is relevant because it ensures that deviations from the true
                    mean decrease rapidly, allowing us to be confident that a single sample mean is a close
                    approximation of the population mean.

                    A common misconception is that the CLT only converges at a sample size of 30. In reality,
                    convergence begins as soon as the sample size increases from 1 to 2. The sample size of 30 is not
                    universally sufficient, especially for unusual distributions. <strong>Cauchy distributions</strong>
                    become a concern when "black swan" events occur.

                    Finally, it's worth noting that the mean minimizes the squared L2 distance between points, while the
                    median minimizes the L1 distance (absolute distance).
                </p>
            </section>

            <section id="significance_tests">
                <h2>Significance Tests</h2>
                <p>This course covers fundamental concepts in Data Science...</p>
            </section>

            <section id="Power">
                <h2>SECTION 3</h2>
                <p>This course covers fundamental concepts in Data Science...</p>
            </section>


            <!-- Add other sections as needed -->
        </main>
    </div>
</body>

</html>